{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing \n",
    "\n",
    "#### Exercise in Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/cookie_reviews.csv',encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize from text  to words to bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_snippets = \"\"\"European authorities fined Google a record 5.1 billion on \n",
    "Wednesday for abusing its power in the mobile phone market.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "1. Tokenize the new snippets into sentences\n",
    "2. For each sentences, further tokenize into uni-grams.\n",
    "3. For each sentence, further tokenize into b-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'authorities', 'fined', 'Google'), ('authorities', 'fined', 'Google', 'a'), ('fined', 'Google', 'a', 'record'), ('Google', 'a', 'record', '5.1'), ('a', 'record', '5.1', 'billion'), ('record', '5.1', 'billion', 'on'), ('5.1', 'billion', 'on', 'Wednesday'), ('billion', 'on', 'Wednesday', 'for'), ('on', 'Wednesday', 'for', 'abusing'), ('Wednesday', 'for', 'abusing', 'its'), ('for', 'abusing', 'its', 'power'), ('abusing', 'its', 'power', 'in'), ('its', 'power', 'in', 'the'), ('power', 'in', 'the', 'mobile'), ('in', 'the', 'mobile', 'phone'), ('the', 'mobile', 'phone', 'market')]\n"
     ]
    }
   ],
   "source": [
    "bigram_mytext = list(ngrams(word_tokenize(my_text),4))\n",
    "print (bigram_mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'authorities'), ('authorities', 'fined'), ('fined', 'Google'), ('Google', 'a'), ('a', 'record'), ('record', '5.1'), ('5.1', 'billion'), ('billion', 'on'), ('on', 'Wednesday'), ('Wednesday', 'for'), ('for', 'abusing'), ('abusing', 'its'), ('its', 'power'), ('power', 'in'), ('in', 'the'), ('the', 'mobile'), ('mobile', 'phone'), ('phone', 'market')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenization in bigrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\u000b",
    "print(twograms)\n",
    "print (twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'authorities', 'fined', 'Google', 'a', 'record', '5.1', 'billion', 'on', 'Wednesday', 'for', 'abusing', 'its', 'power', 'in', 'the', 'mobile', 'phone', 'market']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expression to tokenize using white space\n",
    "\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(whitespace_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'Google', 'Wednesday']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
