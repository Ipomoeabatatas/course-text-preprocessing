{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The COVID-19 virus spreads primarily through droplets of \n",
    "saliva or discharge from the nose when an infected person coughs\n",
    "or sneezes, so it is important that you also practice respiratory etiquette.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tokens_word_pos = pos_tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('COVID-19', 'NNP'), ('virus', 'NN'), ('spreads', 'NNS'), ('primarily', 'RB'), ('through', 'IN'), ('droplets', 'NNS'), ('of', 'IN'), ('saliva', 'NN'), ('or', 'CC'), ('discharge', 'NN'), ('from', 'IN'), ('the', 'DT'), ('nose', 'NN'), ('when', 'WRB'), ('an', 'DT'), ('infected', 'JJ'), ('person', 'NN'), ('coughs', 'NNS'), ('or', 'CC'), ('sneezes', 'NNS'), (',', ','), ('so', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('that', 'IN'), ('you', 'PRP'), ('also', 'RB'), ('practice', 'NN'), ('respiratory', 'NN'), ('etiquette', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (tokens_word_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_word_lem = []\n",
    "\n",
    "for t in tokens_words:\n",
    "   tokens_word_lem.append(lemmatizer.lemmatize(t, pos='n'))\n",
    "\n",
    "### Change line 4 for pos=v to see how the lemmatizer performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spreads  ===> spread\n",
      " droplets  ===> droplet\n",
      " coughs  ===> cough\n",
      " sneezes  ===> sneeze\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens_words)):\n",
    "    if tokens_words[i] != tokens_word_lem[i]:\n",
    "        print (f\" {tokens_words[i]}  ===> {tokens_word_lem[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
